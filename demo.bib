@article{SchleierSmith2021WhatSC,
title = {{What Serverless Computing is and Should Become: The next Phase of Cloud Computing}},
author = {Schleier-Smith, Johann and Sreekanti, Vikram and Khandelwal, Anurag and Carreira, Joao and Yadwadkar, Neeraja J. and Popa, Raluca Ada and Gonzalez, Joseph E. and Stoica, Ion and Patterson, David A.},
year = {2021},
publisher = {Association for Computing Machinery},
abstract = {The evolution that serverless computing represents, the economic forces that shape it, why it could fail, and how it might fulfill its potential.},
journal = {Commun. ACM},
}

@inproceedings{shahradServerlessWildCharacterizing,
  title = {Serverless in the {{Wild}}: {{Characterizing}} and {{Optimizing}} the {{Serverless Workload}} at a {{Large Cloud Provider}}},
  author = {Shahrad, Mohammad and Fonseca, Rodrigo and Goiri, {\'I}{\~n}igo and Chaudhry, Gohar and Batum, Paul and Cooke, Jason and Laureano, Eduardo and Tresness, Colby and Russinovich, Mark and Bianchini, Ricardo},
  abstract = {Function as a Service (FaaS) has been gaining popularity as a way to deploy computations to serverless backends in the cloud. This paradigm shifts the complexity of allocating and provisioning resources to the cloud provider, which has to provide the illusion of always-available resources (i.e., fast function invocations without cold starts) at the lowest possible resource cost. Doing so requires the provider to deeply understand the characteristics of the FaaS workload. Unfortunately, there has been little to no public information on these characteristics. Thus, in this paper, we first characterize the entire production FaaS workload of Azure Functions. We show for example that most functions are invoked very infrequently, but there is an 8-order-of-magnitude range of invocation frequencies. Using observations from our characterization, we then propose a practical resource management policy that significantly reduces the number of function cold starts, while spending fewer resources than state-of-the-practice policies.},
  langid = {english},
  year = {2020},
  series = {USENIX ATC'20}
}

@inproceedings{buyyaSLAorientedResourceProvisioning2011,
  title = {{{SLA-oriented}} Resource Provisioning for Cloud Computing: {{Challenges}}, Architecture, and Solutions},
  shorttitle = {{{SLA-oriented}} Resource Provisioning for Cloud Computing},
  booktitle = {{CSC '11}},
  author = {Buyya, Rajkumar and Garg, Saurabh Kumar and Calheiros, Rodrigo N. and Bla, Bla},
  year = {2011},
  publisher = {{IEEE}},
  abstract = {Cloud computing systems promise to offer subscription-oriented, enterprise-quality computing services to users worldwide. With the increased demand for delivering services to a large number of users, they need to offer differentiated services to users and meet their quality expectations. Existing resource management systems in data centers are yet to support Service Level Agreement (SLA)-oriented resource allocation, and thus need to be enhanced to realize cloud computing and utility computing. In addition, no work has been done to collectively incorporate customer-driven service management, computational risk management, and autonomic resource management into a market-based resource management system to target the rapidly changing enterprise requirements of Cloud computing. This paper presents vision, challenges, and architectural elements of SLA-oriented resource management. The proposed architecture supports integration of marketbased provisioning policies and virtualisation technologies for flexible allocation of resources to applications. The performance results obtained from our working prototype system shows the feasibility and effectiveness of SLA-based resource provisioning in Clouds.},
  langid = {english},
}

@inproceedings{hortaXartrekRuntimeExecution2021,
  title = {{Xar-Trek: Run-Time Execution Migration among FPGAs and Heterogeneous-ISA CPUs}},
  shorttitle = {Xar-Trek},
  booktitle = {{Middleware '22}},
  author = {Horta, Edson and Chuang, Ho-Ren and VSathish, Naarayanan Rao and Philippidis, Cesar and Barbalace, Antonio and Olivier, Pierre and Ravindran, Binoy},
  year = {2021},
  publisher = {{ACM}},
  abstract = {Datacenter servers are increasingly heterogeneous: from x86 host CPUs, to ARM or RISC-V CPUs in NICs/SSDs, to FPGAs. Previous works have demonstrated that migrating application execution at run-time across heterogeneous-ISA CPUs can yield significant performance and energy gains, with relatively little programmer effort. However, FPGAs have often been overlooked in that context: hardware acceleration using FPGAs involves statically implementing select application functions, which prohibits dynamic and transparent migration. We present Xar-Trek, a new compiler and run-time software framework that overcomes this limitation. Xar-Trek compiles an application for several CPU ISAs and select application functions for acceleration on an FPGA, allowing execution migration between heterogeneous-ISA CPUs and FPGAs at run-time. Xar-Trek's run-time monitors server workloads and migrates application functions to an FPGA or to heterogeneous-ISA CPUs based on a scheduling policy. We develop a heuristic policy that uses application workload profiles to make scheduling decisions. Our evaluations conducted on a system with x86-64 server CPUs, ARM64 server CPUs, and an Alveo accelerator card reveal 88\%-1\% performance gains over no-migration baselines.},
  langid = {english}
}

@inproceedings{akkusSANDHighPerformanceServerless,
title = {{SAND: Towards High-Performance Serverless Computing}},
author = {Akkus, Istemi Ekin and Chen, Ruichuan and Rimac, Ivica and Stein, Manuel and Satzke, Klaus and Beck, Andre and Aditya, Paarijaat and Hilt, Volker},
year = {2018},
abstract = {Serverless computing has emerged as a new cloud computing paradigm, where an application consists of individual functions that can be separately managed and executed. However, existing serverless platforms normally isolate and execute functions in separate containers, and do not exploit the interactions among functions for performance. These practices lead to high startup delays for function executions and inefficient resource usage.This paper presents SAND, a new serverless computing system that provides lower latency, better resource efficiency and more elasticity than existing serverless platforms. To achieve these properties, SAND introduces two key techniques: 1) application-level sandboxing, and 2) a hierarchical message bus. We have implemented and deployed a complete SAND system. Our results show that SAND outperforms the state-of-the-art serverless platforms significantly. For example, in a commonly-used image processing application, SAND achieves a 43\% speedup compared to Apache OpenWhisk.},
series = {USENIX ATC '18}
}

@article{lannurienHeROfakeHeterogeneousResourcesa,
  title = {{{HeROfake}}: {{Heterogeneous Resources Orchestration}} in a {{Serverless Cloud}} \textendash{} {{An Application}} to {{Deepfake Detection}}},
  author = {Lannurien, Vincent and D'Orazio, Laurent and Barais, Olivier and Bernard, Esther and Weppe, Olivier and Beaulieu, Laurent and Kacete, Amine and Paquelet, St√©phane and Boukhobza, Jalil},
  year = {2023},
  series = {CCGRID '23},
  abstract = {Serverless is a trending service model for cloud computing. It shifts a lot of the complexity from customers to service providers. However, current serverless platforms mostly consider the provider's infrastructure as homogeneous, as well as the users' requests. This limits possibilities for the provider to leverage heterogeneity in their infrastructure to improve function response time and reduce energy consumption. We propose a heterogeneity-aware serverless orchestrator for private clouds that consists of two components: the autoscaler allocates heterogeneous hardware resources (CPUs, GPUs, FPGAs) for function replicas, while the scheduler maps function executions to these replicas. Our objective is to guarantee function response time, while enabling the provider to reduce resource usage and energy consumption. This work considers a case study for a deepfake detection application relying on CNN inference. We devised a simulation environment that implements our model and a baseline Knative orchestrator, and evaluated both policies with regard to consolidation of tasks, energy consumption and SLA penalties. Experimental results show that our platform yields substantial gains for all those metrics, with an average of 35\% less energy consumed for function executions while consolidating tasks on less than 40\% of the infrastructure's nodes, and more than 60\% less SLA violations.},
  langid = {english},
}

@Inbook{Lannurien2023,
author="Lannurien, Vincent
and D'Orazio, Laurent
and Barais, Olivier
and Boukhobza, Jalil",
title="Serverless Cloud Computing: State of the Art and Challenges",
bookTitle="Serverless Computing: Principles and Paradigms",
year="2023",
abstract="The serverless model represents a paradigm shift in the cloud: as opposed to traditional cloud computing service models, serverless customers do not reserve hardware resources. The execution of their code is event-driven (HTTP requests, cron jobs, etc.) and billing is based on actual resource usage. In return, the responsibility of resource allocation and task placement lies on the provider. While serverless in the wild is mainly advertised as a public cloud offering, solutions are actively developed and backed by solid actors in the industry to allow the development of private cloud serverless platforms. The first generation of serverless offers, ``Function as a Service'' (FaaS), has severe shortcomings that can offset the potential benefits for both customers and providers---in terms of spendings and reliability on the customer side, and in terms of resources multiplexing on the provider side. Circumventing these flaws would allow considerable savings in money and energy for both providers and tenants. This chapter aims at establishing a comprehensive tour of these limitations, and presenting state-of-the-art studies to mitigate weaknesses that are currently holding serverless back from becoming the de facto cloud computing model. The main challenges related to the deployment of such a cloud platform are discussed and some perspectives for future directions in research are given.",
}

@inproceedings{yanHermesEfficientCache2020,
  title = {Hermes: {{Efficient Cache Management}} for {{Container-based Serverless Computing}}},
  shorttitle = {Hermes},
  booktitle = {12th {{Asia-Pacific Symposium}} on {{Internetware}}},
  author = {Yan, Bowen and Gao, Heran and Wu, Heng and Zhang, Wenbo and Hua, Lei and Huang, Tao},
  year = {2020},
  publisher = {{ACM}},
  address = {{Singapore}},
  abstract = {Serverless computing systems are shifting towards shorter function durations and larger degrees of parallelism to eliminate intolerable latency. For container-based serverless computing, the state-of-theart efforts fail to ensure low latency because on-demand container images reloading from remote storage can increase the data transmission rate and downgrades system performance. In this paper we propose Hermes with a two-level caching mechanism to reduce the latency and minimize data transmission rate when massive serverless workloads arrive. Hermes optimizes memory caching by persisting metadata cache and prolonging the lifetime of file cache to improve the cache efficiency of image files. Instead of reclaiming memory, Hermes uses disk caching to reduce memory usage, and gets a low data transmission rate by reloading from local disk cache. Experiment results show that Hermes can reduce 90\% of the data transmission rate and improve the runtime performance of serverless workloads up to 5\texttimes{} in a machine with 300 concurrent containers compared to state-of-the-art efforts.},
  langid = {english},
}

@article{wawrzoniakBoxerDataAnalytics2021a,
  title = {Boxer: {{Data Analytics}} on {{Network-enabled Serverless Platforms}}},
  shorttitle = {Boxer},
  author = {Wawrzoniak, Mike and M{\"u}ller, Ingo and Fraga Barcelos Paulus Bruno, Rodrigo and Alonso, Gustavo},
  year = {2021},
  publisher = {{ETH Zurich}},
  abstract = {Serverless is an attractive platform for a variety of applications in the cloud due to its promise of elasticity, low cost, and fast deployment. Instead of using traditional virtual machine services and a fixed infrastructure, which incurs considerable costs to operate and run, Function-as-a-Service allows triggering short computations on demand with the cost proportional to the time the functions are running. As appealing as the idea is, recent work has shown that for data processing applications (regardless of whether it is OLTP, OLAP, or ML) existing serverless platforms are inadequate and additional services are needed in practice, often to address the lack of communication capabilities between functions. In this paper, we demonstrate how to enable function-to-function communication using conventional TCP/IP and show how the ability to communicate can be used to implement data processing on serverless platforms in a more efficient manner than it was possible until now. Our benchmarks show a speedup as high as 11 \texttimes{} in TPC-H queries over systems that use cloud storage to communicate across functions, sustained function-to-function throughput of 621 Mbit/s, and a round-trip latency of less than 1 ms.},
  langid = {english},
}

% SotA %

@inproceedings{abdiPaletteLoadBalancing2023,
  title        = {{Palette Load Balancing: Locality Hints for Serverless Functions}},
  shorttitle   = {Palette {{Load Balancing}}},
  author       = {Abdi, Mania and Ginzburg, Samuel and Lin, Xiayue Charles and Faleiro, Jose and Chaudhry, Gohar Irfan and Goiri, Inigo and Bianchini, Ricardo and Berger, Daniel S and Fonseca, Rodrigo},
  year         = 2023,
  month        = may,
  booktitle    = {{EuroSys '23}},
  publisher    = {{ACM}},
  address      = {{Rome Italy}},
  pages        = {365--380},
  doi          = {10.1145/3552326.3567496},
  isbn         = {978-1-4503-9487-1},
  urldate      = {2023-05-15},
  abstract     = {Function-as-a-Service (FaaS) serverless computing enables a simple programming model with almost unbounded elasticity. Unfortunately, current FaaS platforms achieve this flexibility at the cost of lower performance for data-intensive applications compared to a serverful deployment. The ability to have computation close to data is a key missing feature. We introduce Palette load balancing, which offers FaaS applications a simple mechanism to express locality to the platform, through hints we term ``colors''. Palette maintains the serverless nature of the service \textendash{} users are still not allocating resources \textendash{} while allowing the platform to place successive invocations related to each other on the same executing node. We compare a prototype of the Palette load balancer to a state-of-the-art locality-oblivious load balancer on representative examples of three applications. For a serverless web application with a local cache, Palette improves the hit ratio by 6x. For a serverless version of Dask, Palette improves run times by 46\% and 40\% on Task Bench and TPC-H, respectively. On a serverless version of NumS, Palette improves run times by 37\%. These improvements largely bridge the gap to serverful implementation of the same systems.},
  langid       = {english}
}
@inproceedings{zijunFassflowEfficient2022,
  title        = {{FaaSFlow: Enable Efficient Workflow Execution for Function-as-a-Service}},
  author       = {Li, Zijun and Liu, Yushi and Guo, Linsong and Chen, Quan and Cheng, Jiagan and Zheng, Wenli and Guo, Minyi},
  year         = 2022,
  booktitle    = {{{ASPLOS '22}}},
  location     = {Lausanne, Switzerland},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  pages        = {782‚Äì796},
  doi          = {10.1145/3503222.3507717},
  isbn         = 9781450392051,
  abstract     = {Serverless computing (Function-as-a-Service) provides fine-grain resource sharing by running functions (or Lambdas) in containers. Data-dependent functions are required to be invoked following a pre-defined logic, which is known as serverless workflows. However, our investigation shows that the traditional master-worker based workflow execution architecture performs poorly in serverless context. One significant overhead results from the master-side workflow schedule pattern, with which the functions are triggered in the master node and assigned to worker nodes for execution. Besides, the data movement between workers also reduces the throughput. To this end, we present a worker-side workflow schedule pattern for serverless workflow execution. Following the design, we implement FaaSFlow to enable efficient workflow execution in the serverless context. Besides, we propose an adaptive storage library FaaStore that enables fast data transfer between functions on the same node without through the database. Experiment results show that FaaSFlow effectively mitigates the workflow scheduling overhead by 74.6\% on average and data transmission overhead by 95\% at most. When the network bandwidth fluctuates, FaaSFlow-FaaStore reduces the throughput degradation by 23.0\%, and is able to multiply the utilization of network bandwidth by 1.5X-4X.},
  numpages     = 15,
  keywords     = {graph partition, master-worker, FaaS, serverless workflows}
}
@inproceedings{bhasiCypressInputSizesensitive2022,
  title        = {{Cypress: Input Size-Sensitive Container Provisioning and Request Scheduling for Serverless Platforms}},
  shorttitle   = {Cypress},
  author       = {Bhasi, Vivek M. and Gunasekaran, Jashwant Raj and Sharma, Aakash and Kandemir, Mahmut Taylan and Das, Chita},
  year         = 2022,
  month        = nov,
  booktitle    = {{SoCC '22}},
  publisher    = {{ACM}},
  address      = {{San Francisco California}},
  pages        = {257--272},
  doi          = {10.1145/3542929.3563464},
  isbn         = {978-1-4503-9414-7},
  urldate      = {2023-03-08},
  abstract     = {The growing popularity of the serverless platform has seen an increase in the number and variety of applications (apps) being deployed on it. The majority of these apps process user-provided input to produce the desired results. Existing work in the area of input-sensitive profiling has empirically shown that many such apps have input size\textendash dependent execution times which can be determined through modelling techniques. Nevertheless, existing serverless resource management frameworks are agnostic to the input size\textendash sensitive nature of these apps. We demonstrate in this paper that this can potentially lead to container over-provisioning and/or end-to-end Service Level Objective (SLO) violations. To address this, we propose Cypress, an input size\textendash sensitive resource management framework, that minimizes the containers provisioned for apps, while ensuring a high degree of SLO compliance. We perform an extensive evaluation of Cypress on top of a Kubernetes-managed cluster using 5 apps from the AWS Serverless Application Repository and/or OpenFaaS Function Store with real-world traces and varied input size distributions. Our experimental results show that Cypress spawns up to 66\% fewer containers, thereby, improving container utilization and saving cluster-wide energy by up to 2.95\texttimes{} and 23\%, respectively, versus state-of-the-art frameworks, while remaining highly SLO-compliant (up to 99.99\%). CCS Concepts \textbullet{} Computer systems organization \textrightarrow{} Cloud Computing; Resource-Management; Scheduling.},
  langid       = {english}
}
@inproceedings{zhangFIRSTExploitingMultiDimensional2023,
  title        = {{FIRST: Exploiting the Multi-Dimensional Attributes of Functions for Power-Aware Serverless Computing}},
  shorttitle   = {{{FIRST}}},
  author       = {Zhang, Lu and Li, Chao and Wang, Xinkai and Feng, Weiqi and Yu, Zheng and Chen, Quan and Leng, Jingwen and Guo, Minyi and Yang, Pu and Yue, Shang},
  year         = 2023,
  month        = may,
  booktitle    = {{IPDPS 2023}},
  publisher    = {{IEEE}},
  address      = {{St. Petersburg, FL, USA}},
  pages        = {864--874},
  doi          = {10.1109/IPDPS54959.2023.00091},
  isbn         = 9798350337662,
  urldate      = {2023-12-08},
  abstract     = {Emerging cloud-native development models raise new challenges for managing server performance and power at microsecond scale. Compared with traditional cloud workloads, serverless functions exhibit unprecedented heterogeneity, variability, and dynamicity. Designing cloud-native power management schemes for serverless functions requires significant engineering effort. Current solutions remain sub-optimal since their orchestration process is often one-sided, lacking a systematic view. A key obstacle to truly efficient function deployment is the fundamental wide abstraction gap between the upper-layer request scheduling and the low-level hardware execution.},
  langid       = {english}
}
@inproceedings{smithFaDOFaaSFunctions2022,
  title        = {{FaDO: FaaS Functions and Data Orchestrator for Multiple Serverless Edge-Cloud Clusters}},
  shorttitle   = {{{FaDO}}},
  author       = {Smith, Christopher Peter and Jindal, Anshul and Chadha, Mohak and Gerndt, Michael and Benedict, Shajulin},
  year         = 2022,
  month        = may,
  booktitle    = {{ICFEC 2022}},
  publisher    = {{IEEE}},
  address      = {{Messina, Italy}},
  pages        = {17--25},
  doi          = {10.1109/ICFEC54809.2022.00010},
  isbn         = {978-1-66549-524-0},
  urldate      = {2023-12-11},
  abstract     = {Function-as-a-Service (FaaS) is an attractive cloud computing model that simplifies application development and deployment. However, current serverless compute platforms do not consider data placement when scheduling functions. With the growing demand for edge-cloud continuum, multi-cloud, and multi-serverless applications, this flaw means serverless technologies are still ill-suited to latency-sensitive operations like media streaming. This work proposes a solution by presenting a tool called FaDO: FaaS Functions and Data Orchestrator, designed to allow data-aware functions scheduling across multiserverless compute clusters present at different locations, such as at the edge and in the cloud. FaDO works through headerbased HTTP reverse proxying and uses three load-balancing algorithms: 1) The Least Connections, 2) Round Robin, and 3) Random for load balancing the invocations of the function across the suitable serverless compute clusters based on the set storage policies. FaDO further provides users with an abstraction of the serverless compute cluster's storage, allowing users to interact with data across different storage services through a unified interface. In addition, users can configure automatic and policy-aware granular data replications, causing FaDO to spread data across the clusters while respecting location constraints. Load testing results show that it is capable of load balancing high-throughput workloads, placing functions near their data without contributing any significant performance overhead.},
  langid       = {english}
}
@article{burckhardtNetheriteEfficientExecution,
  title        = {{Netherite: Efficient Execution of Serverless Workflows}},
  author       = {Burckhardt, Sebastian and Chandramouli, Badrish and Gillum, Chris and Justo, David and Kallas, Konstantinos and McMahon, Connor and Meiklejohn, Christopher S. and Zhu, Xiangfeng},
  year         = 2022,
  month        = {apr},
  journal      = {Proc. VLDB Endow.},
  publisher    = {VLDB Endowment},
  volume       = 15,
  number       = 8,
  pages        = {1591‚Äì1604},
  doi          = {10.14778/3529337.3529344},
  issn         = {2150-8097},
  issue_date   = {April 2022},
  abstract     = {Serverless is a popular choice for cloud service architects because it can provide scalability and load-based billing with minimal developer effort. Functions-as-a-service (FaaS) are originally stateless, but emerging frameworks add stateful abstractions. For instance, the widely used Durable Functions (DF) allow developers to write advanced serverless applications, including reliable workflows and actors, in a programming language of choice. DF implicitly and continuosly persists the state and progress of applications, which greatly simplifies development, but can create an IOps bottleneck.To improve efficiency, we introduce Netherite, a novel architecture for executing serverless workflows on an elastic cluster. Netherite groups the numerous application objects into a smaller number of partitions, and pipelines the state persistence of each partition. This improves latency and throughput, as it enables workflow steps to group commit, even if causally dependent. Moreover, Netherite leverages FASTER's hybrid log approach to support larger-than-memory application state, and to enable efficient partition movement between compute hosts.Our evaluation shows that (a) Netherite achieves lower latency and higher throughput than the original DF engine, by more than an order of magnitude in some cases, and (b) that Netherite has lower latency than some commonly used alternatives, like AWS Step Functions or cloud storage triggers.},
  numpages     = 14
}
@inproceedings{fuerstIluvatarFastControl2023,
  title        = {{Il\'{u}vatar: A Fast Control Plane for Serverless Computing}},
  author       = {Fuerst, Alexander and Rehman, Abdul and Sharma, Prateek},
  year         = 2023,
  booktitle    = {{HPDC '23}},
  location     = {Orlando, FL, USA},
  publisher    = {Association for Computing Machinery},
  address      = {New York, NY, USA},
  pages        = {267‚Äì280},
  doi          = {10.1145/3588195.3592995},
  isbn         = 9798400701559,
  abstract     = {Providing efficient Functions as a Service (FaaS) is challenging due to the serverless programming model and highly heterogeneous and dynamic workloads. Great strides have been made in optimizing FaaS performance through scheduling, caching, virtualization, and other resource management techniques. The combination of these advances and growing FaaS workloads have pushed the performance bottleneck into the control plane itself. Current FaaS control planes like OpenWhisk introduce 100s of milliseconds of latency overhead, and are becoming unsuitable for high performance FaaS research and deployments.We present the design and implementation of Il\'{u}vatar, a fast, modular, extensible FaaS control plane which reduces the latency overhead by more than two orders of magnitude. Il\'{u}vatar has a worker-centric architecture and introduces a new function queue technique for managing function scheduling and overcommitment. Il\'{u}vatar is implemented in Rust in about 13,000 lines of code, and introduces only 3ms of latency overhead under a wide range of loads, which is more than 2 orders of magnitude lower than OpenWhisk.},
  numpages     = 14,
  keywords     = {open source, functions as a service, cloud computing, serverless computing}
}